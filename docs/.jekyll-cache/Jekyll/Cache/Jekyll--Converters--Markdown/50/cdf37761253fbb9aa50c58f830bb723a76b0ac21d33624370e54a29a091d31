I"½<p>Spark&#8217;s support for Hadoop InputFormat allows it to process data in OpenStack Swift using the
same URI formats as in Hadoop. You can specify a path in Swift as input through a 
URI of the form <code>swift://container.PROVIDER/path</code>. You will also need to set your 
Swift security credentials, through <code>core-site.xml</code> or via
<code>SparkContext.hadoopConfiguration</code>.
The current Swift driver requires Swift to use the Keystone authentication method, or
its Rackspace-specific predecessor.</p>

<h1 id="configuring-swift-for-better-data-locality">Configuring Swift for Better Data Locality</h1>

<p>Although not mandatory, it is recommended to configure the proxy server of Swift with
<code>list_endpoints</code> to have better data locality. More information is
<a href="https://github.com/openstack/swift/blob/master/swift/common/middleware/list_endpoints.py">available here</a>.</p>

<h1 id="dependencies">Dependencies</h1>

<p>The Spark application should include <code>hadoop-openstack</code> dependency, which can
be done by including the <code>hadoop-cloud</code> module for the specific version of spark used.
For example, for Maven support, add the following to the <code>pom.xml</code> file:</p>

<figure class="highlight"><pre><code class="language-xml" data-lang="xml"><span class="nt">&lt;dependencyManagement&gt;</span>
  ...
  <span class="nt">&lt;dependency&gt;</span>
    <span class="nt">&lt;groupId&gt;</span>org.apache.spark<span class="nt">&lt;/groupId&gt;</span>
    <span class="nt">&lt;artifactId&gt;</span>hadoop-cloud_2.12<span class="nt">&lt;/artifactId&gt;</span>
    <span class="nt">&lt;version&gt;</span>${spark.version}<span class="nt">&lt;/version&gt;</span>
  <span class="nt">&lt;/dependency&gt;</span>
  ...
<span class="nt">&lt;/dependencyManagement&gt;</span></code></pre></figure>

<h1 id="configuration-parameters">Configuration Parameters</h1>

<p>Create <code>core-site.xml</code> and place it inside Spark&#8217;s <code>conf</code> directory.
The main category of parameters that should be configured is the authentication parameters
required by Keystone.</p>

<p>The following table contains a list of Keystone mandatory parameters. <code>PROVIDER</code> can be
any (alphanumeric) name.</p>

<table class="table">
<tr><th>Property Name</th><th>Meaning</th><th>Required</th></tr>
<tr>
  <td><code>fs.swift.service.PROVIDER.auth.url</code></td>
  <td>Keystone Authentication URL</td>
  <td>Mandatory</td>
</tr>
<tr>
  <td><code>fs.swift.service.PROVIDER.auth.endpoint.prefix</code></td>
  <td>Keystone endpoints prefix</td>
  <td>Optional</td>
</tr>
<tr>
  <td><code>fs.swift.service.PROVIDER.tenant</code></td>
  <td>Tenant</td>
  <td>Mandatory</td>
</tr>
<tr>
  <td><code>fs.swift.service.PROVIDER.username</code></td>
  <td>Username</td>
  <td>Mandatory</td>
</tr>
<tr>
  <td><code>fs.swift.service.PROVIDER.password</code></td>
  <td>Password</td>
  <td>Mandatory</td>
</tr>
<tr>
  <td><code>fs.swift.service.PROVIDER.http.port</code></td>
  <td>HTTP port</td>
  <td>Mandatory</td>
</tr>
<tr>
  <td><code>fs.swift.service.PROVIDER.region</code></td>
  <td>Keystone region</td>
  <td>Mandatory</td>
</tr>
<tr>
  <td><code>fs.swift.service.PROVIDER.public</code></td>
  <td>Indicates whether to use the public (off cloud) or private (in cloud; no transfer fees) endpoints</td>
  <td>Mandatory</td>
</tr>
</table>

<p>For example, assume <code>PROVIDER=SparkTest</code> and Keystone contains user <code>tester</code> with password <code>testing</code>
defined for tenant <code>test</code>. Then <code>core-site.xml</code> should include:</p>

<figure class="highlight"><pre><code class="language-xml" data-lang="xml"><span class="nt">&lt;configuration&gt;</span>
  <span class="nt">&lt;property&gt;</span>
    <span class="nt">&lt;name&gt;</span>fs.swift.service.SparkTest.auth.url<span class="nt">&lt;/name&gt;</span>
    <span class="nt">&lt;value&gt;</span>http://127.0.0.1:5000/v2.0/tokens<span class="nt">&lt;/value&gt;</span>
  <span class="nt">&lt;/property&gt;</span>
  <span class="nt">&lt;property&gt;</span>
    <span class="nt">&lt;name&gt;</span>fs.swift.service.SparkTest.auth.endpoint.prefix<span class="nt">&lt;/name&gt;</span>
    <span class="nt">&lt;value&gt;</span>endpoints<span class="nt">&lt;/value&gt;</span>
  <span class="nt">&lt;/property&gt;</span>
    <span class="nt">&lt;name&gt;</span>fs.swift.service.SparkTest.http.port<span class="nt">&lt;/name&gt;</span>
    <span class="nt">&lt;value&gt;</span>8080<span class="nt">&lt;/value&gt;</span>
  <span class="nt">&lt;/property&gt;</span>
  <span class="nt">&lt;property&gt;</span>
    <span class="nt">&lt;name&gt;</span>fs.swift.service.SparkTest.region<span class="nt">&lt;/name&gt;</span>
    <span class="nt">&lt;value&gt;</span>RegionOne<span class="nt">&lt;/value&gt;</span>
  <span class="nt">&lt;/property&gt;</span>
  <span class="nt">&lt;property&gt;</span>
    <span class="nt">&lt;name&gt;</span>fs.swift.service.SparkTest.public<span class="nt">&lt;/name&gt;</span>
    <span class="nt">&lt;value&gt;</span>true<span class="nt">&lt;/value&gt;</span>
  <span class="nt">&lt;/property&gt;</span>
  <span class="nt">&lt;property&gt;</span>
    <span class="nt">&lt;name&gt;</span>fs.swift.service.SparkTest.tenant<span class="nt">&lt;/name&gt;</span>
    <span class="nt">&lt;value&gt;</span>test<span class="nt">&lt;/value&gt;</span>
  <span class="nt">&lt;/property&gt;</span>
  <span class="nt">&lt;property&gt;</span>
    <span class="nt">&lt;name&gt;</span>fs.swift.service.SparkTest.username<span class="nt">&lt;/name&gt;</span>
    <span class="nt">&lt;value&gt;</span>tester<span class="nt">&lt;/value&gt;</span>
  <span class="nt">&lt;/property&gt;</span>
  <span class="nt">&lt;property&gt;</span>
    <span class="nt">&lt;name&gt;</span>fs.swift.service.SparkTest.password<span class="nt">&lt;/name&gt;</span>
    <span class="nt">&lt;value&gt;</span>testing<span class="nt">&lt;/value&gt;</span>
  <span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;/configuration&gt;</span></code></pre></figure>

<p>Notice that
<code>fs.swift.service.PROVIDER.tenant</code>,
<code>fs.swift.service.PROVIDER.username</code>, 
<code>fs.swift.service.PROVIDER.password</code> contains sensitive information and keeping them in
<code>core-site.xml</code> is not always a good approach.
We suggest to keep those parameters in <code>core-site.xml</code> for testing purposes when running Spark
via <code>spark-shell</code>.
For job submissions they should be provided via <code>sparkContext.hadoopConfiguration</code>.</p>
:ET