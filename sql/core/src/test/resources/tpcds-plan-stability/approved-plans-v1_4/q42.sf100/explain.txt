== Parsed Logical Plan ==
'GlobalLimit 100
+- 'LocalLimit 100
   +- 'Sort ['sum('ss_ext_sales_price) DESC NULLS LAST, 'dt.d_year ASC NULLS FIRST, 'item.i_category_id ASC NULLS FIRST, 'item.i_category ASC NULLS FIRST], true
      +- 'Aggregate ['dt.d_year, 'item.i_category_id, 'item.i_category], ['dt.d_year, 'item.i_category_id, 'item.i_category, unresolvedalias('sum('ss_ext_sales_price), None)]
         +- 'Filter (((('dt.d_date_sk = 'store_sales.ss_sold_date_sk) AND ('store_sales.ss_item_sk = 'item.i_item_sk)) AND ('item.i_manager_id = 1)) AND (('dt.d_moy = 11) AND ('dt.d_year = 2000)))
            +- 'Join Inner
               :- 'Join Inner
               :  :- 'SubqueryAlias dt
               :  :  +- 'UnresolvedRelation [date_dim]
               :  +- 'UnresolvedRelation [store_sales]
               +- 'UnresolvedRelation [item]

== Analyzed Logical Plan ==
d_year: int, i_category_id: int, i_category: string, sum(ss_ext_sales_price): decimal(17,2)
GlobalLimit 100
+- LocalLimit 100
   +- Project [d_year#1, i_category_id#2, i_category#3, sum(ss_ext_sales_price)#4]
      +- Sort [sum(ss_ext_sales_price)#4 DESC NULLS LAST, d_year#1 ASC NULLS FIRST, i_category_id#2 ASC NULLS FIRST, i_category#3 ASC NULLS FIRST], true
         +- Aggregate [d_year#1, i_category_id#2, i_category#3], [d_year#1, i_category_id#2, i_category#3, sum(ss_ext_sales_price#5) AS sum(ss_ext_sales_price)#4]
            +- Filter ((((d_date_sk#6 = ss_sold_date_sk#7) AND (ss_item_sk#8 = i_item_sk#9)) AND (i_manager_id#10 = 1)) AND ((d_moy#11 = 11) AND (d_year#1 = 2000)))
               +- Join Inner
                  :- Join Inner
                  :  :- SubqueryAlias dt
                  :  :  +- SubqueryAlias spark_catalog.default.date_dim
                  :  :     +- Relation[d_date_sk#6,d_date_id#12,d_date#13,d_month_seq#14,d_week_seq#15,d_quarter_seq#16,d_year#1,d_dow#17,d_moy#11,d_dom#18,d_qoy#19,d_fy_year#20,d_fy_quarter_seq#21,d_fy_week_seq#22,d_day_name#23,d_quarter_name#24,d_holiday#25,d_weekend#26,d_following_holiday#27,d_first_dom#28,d_last_dom#29,d_same_day_ly#30,d_same_day_lq#31,d_current_day#32,... 4 more fields] parquet
                  :  +- SubqueryAlias spark_catalog.default.store_sales
                  :     +- Relation[ss_sold_date_sk#7,ss_sold_time_sk#33,ss_item_sk#8,ss_customer_sk#34,ss_cdemo_sk#35,ss_hdemo_sk#36,ss_addr_sk#37,ss_store_sk#38,ss_promo_sk#39,ss_ticket_number#40,ss_quantity#41,ss_wholesale_cost#42,ss_list_price#43,ss_sales_price#44,ss_ext_discount_amt#45,ss_ext_sales_price#5,ss_ext_wholesale_cost#46,ss_ext_list_price#47,ss_ext_tax#48,ss_coupon_amt#49,ss_net_paid#50,ss_net_paid_inc_tax#51,ss_net_profit#52] parquet
                  +- SubqueryAlias spark_catalog.default.item
                     +- Relation[i_item_sk#9,i_item_id#53,i_rec_start_date#54,i_rec_end_date#55,i_item_desc#56,i_current_price#57,i_wholesale_cost#58,i_brand_id#59,i_brand#60,i_class_id#61,i_class#62,i_category_id#2,i_category#3,i_manufact_id#63,i_manufact#64,i_size#65,i_formulation#66,i_color#67,i_units#68,i_container#69,i_manager_id#10,i_product_name#70] parquet

== Optimized Logical Plan ==
GlobalLimit 100
+- LocalLimit 100
   +- Sort [sum(ss_ext_sales_price)#4 DESC NULLS LAST, d_year#1 ASC NULLS FIRST, i_category_id#2 ASC NULLS FIRST, i_category#3 ASC NULLS FIRST], true
      +- Aggregate [d_year#1, i_category_id#2, i_category#3], [d_year#1, i_category_id#2, i_category#3, MakeDecimal(sum(UnscaledValue(ss_ext_sales_price#5)),17,2) AS sum(ss_ext_sales_price)#4]
         +- Project [d_year#1, ss_ext_sales_price#5, i_category_id#2, i_category#3]
            +- Join Inner, (d_date_sk#6 = ss_sold_date_sk#7)
               :- Project [ss_sold_date_sk#7, ss_ext_sales_price#5, i_category_id#2, i_category#3]
               :  +- Join Inner, (ss_item_sk#8 = i_item_sk#9)
               :     :- Project [ss_sold_date_sk#7, ss_item_sk#8, ss_ext_sales_price#5]
               :     :  +- Filter (isnotnull(ss_sold_date_sk#7) AND isnotnull(ss_item_sk#8))
               :     :     +- Relation[ss_sold_date_sk#7,ss_sold_time_sk#33,ss_item_sk#8,ss_customer_sk#34,ss_cdemo_sk#35,ss_hdemo_sk#36,ss_addr_sk#37,ss_store_sk#38,ss_promo_sk#39,ss_ticket_number#40,ss_quantity#41,ss_wholesale_cost#42,ss_list_price#43,ss_sales_price#44,ss_ext_discount_amt#45,ss_ext_sales_price#5,ss_ext_wholesale_cost#46,ss_ext_list_price#47,ss_ext_tax#48,ss_coupon_amt#49,ss_net_paid#50,ss_net_paid_inc_tax#51,ss_net_profit#52] parquet
               :     +- Project [i_item_sk#9, i_category_id#2, i_category#3]
               :        +- Filter ((isnotnull(i_manager_id#10) AND (i_manager_id#10 = 1)) AND isnotnull(i_item_sk#9))
               :           +- Relation[i_item_sk#9,i_item_id#53,i_rec_start_date#54,i_rec_end_date#55,i_item_desc#56,i_current_price#57,i_wholesale_cost#58,i_brand_id#59,i_brand#60,i_class_id#61,i_class#62,i_category_id#2,i_category#3,i_manufact_id#63,i_manufact#64,i_size#65,i_formulation#66,i_color#67,i_units#68,i_container#69,i_manager_id#10,i_product_name#70] parquet
               +- Project [d_date_sk#6, d_year#1]
                  +- Filter ((((isnotnull(d_moy#11) AND isnotnull(d_year#1)) AND (d_moy#11 = 11)) AND (d_year#1 = 2000)) AND isnotnull(d_date_sk#6))
                     +- Relation[d_date_sk#6,d_date_id#12,d_date#13,d_month_seq#14,d_week_seq#15,d_quarter_seq#16,d_year#1,d_dow#17,d_moy#11,d_dom#18,d_qoy#19,d_fy_year#20,d_fy_quarter_seq#21,d_fy_week_seq#22,d_day_name#23,d_quarter_name#24,d_holiday#25,d_weekend#26,d_following_holiday#27,d_first_dom#28,d_last_dom#29,d_same_day_ly#30,d_same_day_lq#31,d_current_day#32,... 4 more fields] parquet

== Physical Plan ==
TakeOrderedAndProject(limit=100, orderBy=[sum(ss_ext_sales_price)#4 DESC NULLS LAST,d_year#1 ASC NULLS FIRST,i_category_id#2 ASC NULLS FIRST,i_category#3 ASC NULLS FIRST], output=[d_year#1,i_category_id#2,i_category#3,sum(ss_ext_sales_price)#4])
+- *(4) HashAggregate(keys=[d_year#1, i_category_id#2, i_category#3], functions=[sum(UnscaledValue(ss_ext_sales_price#5))], output=[d_year#1, i_category_id#2, i_category#3, sum(ss_ext_sales_price)#4])
   +- Exchange hashpartitioning(d_year#1, i_category_id#2, i_category#3, 5), true, [id=#71]
      +- *(3) HashAggregate(keys=[d_year#1, i_category_id#2, i_category#3], functions=[partial_sum(UnscaledValue(ss_ext_sales_price#5))], output=[d_year#1, i_category_id#2, i_category#3, sum#72])
         +- *(3) Project [d_year#1, ss_ext_sales_price#5, i_category_id#2, i_category#3]
            +- *(3) BroadcastHashJoin [ss_sold_date_sk#7], [d_date_sk#6], Inner, BuildRight, false
               :- *(3) Project [ss_sold_date_sk#7, ss_ext_sales_price#5, i_category_id#2, i_category#3]
               :  +- *(3) BroadcastHashJoin [ss_item_sk#8], [i_item_sk#9], Inner, BuildRight, false
               :     :- *(3) Filter (isnotnull(ss_sold_date_sk#7) AND isnotnull(ss_item_sk#8))
               :     :  +- *(3) ColumnarToRow
               :     :     +- FileScan parquet default.store_sales[ss_sold_date_sk#7,ss_item_sk#8,ss_ext_sales_price#5] Batched: true, DataFilters: [isnotnull(ss_sold_date_sk#7), isnotnull(ss_item_sk#8)], Format: Parquet, Location: InMemoryFileIndex[file:/Users/yi.wu/IdeaProjects/spark/sql/core/spark-warehouse/org.apache.spark...., PartitionFilters: [], PushedFilters: [IsNotNull(ss_sold_date_sk), IsNotNull(ss_item_sk)], ReadSchema: struct<ss_sold_date_sk:int,ss_item_sk:int,ss_ext_sales_price:decimal(7,2)>
               :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [id=#73]
               :        +- *(1) Project [i_item_sk#9, i_category_id#2, i_category#3]
               :           +- *(1) Filter ((isnotnull(i_manager_id#10) AND (i_manager_id#10 = 1)) AND isnotnull(i_item_sk#9))
               :              +- *(1) ColumnarToRow
               :                 +- FileScan parquet default.item[i_item_sk#9,i_category_id#2,i_category#3,i_manager_id#10] Batched: true, DataFilters: [isnotnull(i_manager_id#10), (i_manager_id#10 = 1), isnotnull(i_item_sk#9)], Format: Parquet, Location: InMemoryFileIndex[file:/Users/yi.wu/IdeaProjects/spark/sql/core/spark-warehouse/org.apache.spark...., PartitionFilters: [], PushedFilters: [IsNotNull(i_manager_id), EqualTo(i_manager_id,1), IsNotNull(i_item_sk)], ReadSchema: struct<i_item_sk:int,i_category_id:int,i_category:string,i_manager_id:int>
               +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [id=#74]
                  +- *(2) Project [d_date_sk#6, d_year#1]
                     +- *(2) Filter ((((isnotnull(d_moy#11) AND isnotnull(d_year#1)) AND (d_moy#11 = 11)) AND (d_year#1 = 2000)) AND isnotnull(d_date_sk#6))
                        +- *(2) ColumnarToRow
                           +- FileScan parquet default.date_dim[d_date_sk#6,d_year#1,d_moy#11] Batched: true, DataFilters: [isnotnull(d_moy#11), isnotnull(d_year#1), (d_moy#11 = 11), (d_year#1 = 2000), isnotnul..., Format: Parquet, Location: InMemoryFileIndex[file:/Users/yi.wu/IdeaProjects/spark/sql/core/spark-warehouse/org.apache.spark...., PartitionFilters: [], PushedFilters: [IsNotNull(d_moy), IsNotNull(d_year), EqualTo(d_moy,11), EqualTo(d_year,2000), IsNotNull(d_date_sk)], ReadSchema: struct<d_date_sk:int,d_year:int,d_moy:int>
