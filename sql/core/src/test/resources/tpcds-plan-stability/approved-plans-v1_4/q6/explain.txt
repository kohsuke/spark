== Parsed Logical Plan ==
'GlobalLimit 100
+- 'LocalLimit 100
   +- 'Sort ['cnt ASC NULLS FIRST], true
      +- 'UnresolvedHaving ('count(1) >= 10)
         +- 'Aggregate ['a.ca_state], ['a.ca_state AS state#1, 'count(1) AS cnt#2]
            +- 'Filter (((('a.ca_address_sk = 'c.c_current_addr_sk) AND ('c.c_customer_sk = 's.ss_customer_sk)) AND ('s.ss_sold_date_sk = 'd.d_date_sk)) AND ((('s.ss_item_sk = 'i.i_item_sk) AND ('d.d_month_seq = scalar-subquery#3 [])) AND ('i.i_current_price > (1.2 * scalar-subquery#4 []))))
               :  :- 'Distinct
               :  :  +- 'Project ['d_month_seq]
               :  :     +- 'Filter (('d_year = 2000) AND ('d_moy = 1))
               :  :        +- 'UnresolvedRelation [date_dim]
               :  +- 'Project [unresolvedalias('avg('j.i_current_price), None)]
               :     +- 'Filter ('j.i_category = 'i.i_category)
               :        +- 'SubqueryAlias j
               :           +- 'UnresolvedRelation [item]
               +- 'Join Inner
                  :- 'Join Inner
                  :  :- 'Join Inner
                  :  :  :- 'Join Inner
                  :  :  :  :- 'SubqueryAlias a
                  :  :  :  :  +- 'UnresolvedRelation [customer_address]
                  :  :  :  +- 'SubqueryAlias c
                  :  :  :     +- 'UnresolvedRelation [customer]
                  :  :  +- 'SubqueryAlias s
                  :  :     +- 'UnresolvedRelation [store_sales]
                  :  +- 'SubqueryAlias d
                  :     +- 'UnresolvedRelation [date_dim]
                  +- 'SubqueryAlias i
                     +- 'UnresolvedRelation [item]

== Analyzed Logical Plan ==
state: string, cnt: bigint
GlobalLimit 100
+- LocalLimit 100
   +- Sort [cnt#5 ASC NULLS FIRST], true
      +- Project [state#1, cnt#5]
         +- Filter (count(1)#6 >= cast(10 as bigint))
            +- Aggregate [ca_state#7], [ca_state#7 AS state#1, count(1) AS cnt#5, count(1) AS count(1)#6]
               +- Filter ((((ca_address_sk#8 = c_current_addr_sk#9) AND (c_customer_sk#10 = ss_customer_sk#11)) AND (ss_sold_date_sk#12 = d_date_sk#13)) AND (((ss_item_sk#14 = i_item_sk#15) AND (d_month_seq#16 = scalar-subquery#3 [])) AND (cast(i_current_price#17 as decimal(14,7)) > cast(CheckOverflow((promote_precision(cast(1.2 as decimal(11,6))) * promote_precision(cast(scalar-subquery#4 [i_category#18] as decimal(11,6)))), DecimalType(14,7), true) as decimal(14,7)))))
                  :  :- Distinct
                  :  :  +- Project [d_month_seq#16]
                  :  :     +- Filter ((d_year#19 = 2000) AND (d_moy#20 = 1))
                  :  :        +- SubqueryAlias spark_catalog.default.date_dim
                  :  :           +- Relation[d_date_sk#13,d_date_id#21,d_date#22,d_month_seq#16,d_week_seq#23,d_quarter_seq#24,d_year#19,d_dow#25,d_moy#20,d_dom#26,d_qoy#27,d_fy_year#28,d_fy_quarter_seq#29,d_fy_week_seq#30,d_day_name#31,d_quarter_name#32,d_holiday#33,d_weekend#34,d_following_holiday#35,d_first_dom#36,d_last_dom#37,d_same_day_ly#38,d_same_day_lq#39,d_current_day#40,... 4 more fields] parquet
                  :  +- Aggregate [avg(i_current_price#17) AS avg(i_current_price)#41]
                  :     +- Filter (i_category#18 = outer(i_category#18))
                  :        +- SubqueryAlias j
                  :           +- SubqueryAlias spark_catalog.default.item
                  :              +- Relation[i_item_sk#15,i_item_id#42,i_rec_start_date#43,i_rec_end_date#44,i_item_desc#45,i_current_price#17,i_wholesale_cost#46,i_brand_id#47,i_brand#48,i_class_id#49,i_class#50,i_category_id#51,i_category#18,i_manufact_id#52,i_manufact#53,i_size#54,i_formulation#55,i_color#56,i_units#57,i_container#58,i_manager_id#59,i_product_name#60] parquet
                  +- Join Inner
                     :- Join Inner
                     :  :- Join Inner
                     :  :  :- Join Inner
                     :  :  :  :- SubqueryAlias a
                     :  :  :  :  +- SubqueryAlias spark_catalog.default.customer_address
                     :  :  :  :     +- Relation[ca_address_sk#8,ca_address_id#61,ca_street_number#62,ca_street_name#63,ca_street_type#64,ca_suite_number#65,ca_city#66,ca_county#67,ca_state#7,ca_zip#68,ca_country#69,ca_gmt_offset#70,ca_location_type#71] parquet
                     :  :  :  +- SubqueryAlias c
                     :  :  :     +- SubqueryAlias spark_catalog.default.customer
                     :  :  :        +- Relation[c_customer_sk#10,c_customer_id#72,c_current_cdemo_sk#73,c_current_hdemo_sk#74,c_current_addr_sk#9,c_first_shipto_date_sk#75,c_first_sales_date_sk#76,c_salutation#77,c_first_name#78,c_last_name#79,c_preferred_cust_flag#80,c_birth_day#81,c_birth_month#82,c_birth_year#83,c_birth_country#84,c_login#85,c_email_address#86,c_last_review_date#87] parquet
                     :  :  +- SubqueryAlias s
                     :  :     +- SubqueryAlias spark_catalog.default.store_sales
                     :  :        +- Relation[ss_sold_date_sk#12,ss_sold_time_sk#88,ss_item_sk#14,ss_customer_sk#11,ss_cdemo_sk#89,ss_hdemo_sk#90,ss_addr_sk#91,ss_store_sk#92,ss_promo_sk#93,ss_ticket_number#94,ss_quantity#95,ss_wholesale_cost#96,ss_list_price#97,ss_sales_price#98,ss_ext_discount_amt#99,ss_ext_sales_price#100,ss_ext_wholesale_cost#101,ss_ext_list_price#102,ss_ext_tax#103,ss_coupon_amt#104,ss_net_paid#105,ss_net_paid_inc_tax#106,ss_net_profit#107] parquet
                     :  +- SubqueryAlias d
                     :     +- SubqueryAlias spark_catalog.default.date_dim
                     :        +- Relation[d_date_sk#13,d_date_id#21,d_date#22,d_month_seq#16,d_week_seq#23,d_quarter_seq#24,d_year#19,d_dow#25,d_moy#20,d_dom#26,d_qoy#27,d_fy_year#28,d_fy_quarter_seq#29,d_fy_week_seq#30,d_day_name#31,d_quarter_name#32,d_holiday#33,d_weekend#34,d_following_holiday#35,d_first_dom#36,d_last_dom#37,d_same_day_ly#38,d_same_day_lq#39,d_current_day#40,... 4 more fields] parquet
                     +- SubqueryAlias i
                        +- SubqueryAlias spark_catalog.default.item
                           +- Relation[i_item_sk#15,i_item_id#42,i_rec_start_date#43,i_rec_end_date#44,i_item_desc#45,i_current_price#17,i_wholesale_cost#46,i_brand_id#47,i_brand#48,i_class_id#49,i_class#50,i_category_id#51,i_category#18,i_manufact_id#52,i_manufact#53,i_size#54,i_formulation#55,i_color#56,i_units#57,i_container#58,i_manager_id#59,i_product_name#60] parquet

== Optimized Logical Plan ==
GlobalLimit 100
+- LocalLimit 100
   +- Sort [cnt#5 ASC NULLS FIRST], true
      +- Project [state#1, cnt#5]
         +- Filter (count(1)#6 >= 10)
            +- Aggregate [ca_state#7], [ca_state#7 AS state#1, count(1) AS cnt#5, count(1) AS count(1)#6]
               +- Project [ca_state#7]
                  +- Join Inner, (ss_item_sk#14 = i_item_sk#15)
                     :- Project [ca_state#7, ss_item_sk#14]
                     :  +- Join Inner, (ss_sold_date_sk#12 = d_date_sk#13)
                     :     :- Project [ca_state#7, ss_sold_date_sk#12, ss_item_sk#14]
                     :     :  +- Join Inner, (c_customer_sk#10 = ss_customer_sk#11)
                     :     :     :- Project [ca_state#7, c_customer_sk#10]
                     :     :     :  +- Join Inner, (ca_address_sk#8 = c_current_addr_sk#9)
                     :     :     :     :- Project [ca_address_sk#8, ca_state#7]
                     :     :     :     :  +- Filter isnotnull(ca_address_sk#8)
                     :     :     :     :     +- Relation[ca_address_sk#8,ca_address_id#61,ca_street_number#62,ca_street_name#63,ca_street_type#64,ca_suite_number#65,ca_city#66,ca_county#67,ca_state#7,ca_zip#68,ca_country#69,ca_gmt_offset#70,ca_location_type#71] parquet
                     :     :     :     +- Project [c_customer_sk#10, c_current_addr_sk#9]
                     :     :     :        +- Filter (isnotnull(c_current_addr_sk#9) AND isnotnull(c_customer_sk#10))
                     :     :     :           +- Relation[c_customer_sk#10,c_customer_id#72,c_current_cdemo_sk#73,c_current_hdemo_sk#74,c_current_addr_sk#9,c_first_shipto_date_sk#75,c_first_sales_date_sk#76,c_salutation#77,c_first_name#78,c_last_name#79,c_preferred_cust_flag#80,c_birth_day#81,c_birth_month#82,c_birth_year#83,c_birth_country#84,c_login#85,c_email_address#86,c_last_review_date#87] parquet
                     :     :     +- Project [ss_sold_date_sk#12, ss_item_sk#14, ss_customer_sk#11]
                     :     :        +- Filter ((isnotnull(ss_customer_sk#11) AND isnotnull(ss_sold_date_sk#12)) AND isnotnull(ss_item_sk#14))
                     :     :           +- Relation[ss_sold_date_sk#12,ss_sold_time_sk#88,ss_item_sk#14,ss_customer_sk#11,ss_cdemo_sk#89,ss_hdemo_sk#90,ss_addr_sk#91,ss_store_sk#92,ss_promo_sk#93,ss_ticket_number#94,ss_quantity#95,ss_wholesale_cost#96,ss_list_price#97,ss_sales_price#98,ss_ext_discount_amt#99,ss_ext_sales_price#100,ss_ext_wholesale_cost#101,ss_ext_list_price#102,ss_ext_tax#103,ss_coupon_amt#104,ss_net_paid#105,ss_net_paid_inc_tax#106,ss_net_profit#107] parquet
                     :     +- Project [d_date_sk#13]
                     :        +- Filter ((isnotnull(d_month_seq#16) AND (d_month_seq#16 = scalar-subquery#3 [])) AND isnotnull(d_date_sk#13))
                     :           :  +- Aggregate [d_month_seq#16], [d_month_seq#16]
                     :           :     +- Project [d_month_seq#16]
                     :           :        +- Filter (((isnotnull(d_year#19) AND isnotnull(d_moy#20)) AND (d_year#19 = 2000)) AND (d_moy#20 = 1))
                     :           :           +- Relation[d_date_sk#13,d_date_id#21,d_date#22,d_month_seq#16,d_week_seq#23,d_quarter_seq#24,d_year#19,d_dow#25,d_moy#20,d_dom#26,d_qoy#27,d_fy_year#28,d_fy_quarter_seq#29,d_fy_week_seq#30,d_day_name#31,d_quarter_name#32,d_holiday#33,d_weekend#34,d_following_holiday#35,d_first_dom#36,d_last_dom#37,d_same_day_ly#38,d_same_day_lq#39,d_current_day#40,... 4 more fields] parquet
                     :           +- Relation[d_date_sk#13,d_date_id#21,d_date#22,d_month_seq#16,d_week_seq#23,d_quarter_seq#24,d_year#19,d_dow#25,d_moy#20,d_dom#26,d_qoy#27,d_fy_year#28,d_fy_quarter_seq#29,d_fy_week_seq#30,d_day_name#31,d_quarter_name#32,d_holiday#33,d_weekend#34,d_following_holiday#35,d_first_dom#36,d_last_dom#37,d_same_day_ly#38,d_same_day_lq#39,d_current_day#40,... 4 more fields] parquet
                     +- Project [i_item_sk#15]
                        +- Filter (cast(i_current_price#17 as decimal(14,7)) > CheckOverflow((1.200000 * promote_precision(avg(i_current_price)#41)), DecimalType(14,7), true))
                           +- Join LeftOuter, (i_category#18#108 = i_category#18)
                              :- Project [i_item_sk#15, i_current_price#17, i_category#18]
                              :  +- Filter (isnotnull(i_current_price#17) AND isnotnull(i_item_sk#15))
                              :     +- Relation[i_item_sk#15,i_item_id#42,i_rec_start_date#43,i_rec_end_date#44,i_item_desc#45,i_current_price#17,i_wholesale_cost#46,i_brand_id#47,i_brand#48,i_class_id#49,i_class#50,i_category_id#51,i_category#18,i_manufact_id#52,i_manufact#53,i_size#54,i_formulation#55,i_color#56,i_units#57,i_container#58,i_manager_id#59,i_product_name#60] parquet
                              +- Aggregate [i_category#18], [cast((avg(UnscaledValue(i_current_price#17)) / 100.0) as decimal(11,6)) AS avg(i_current_price)#41, i_category#18 AS i_category#18#108]
                                 +- Project [i_current_price#17, i_category#18]
                                    +- Filter isnotnull(i_category#18)
                                       +- Relation[i_item_sk#15,i_item_id#42,i_rec_start_date#43,i_rec_end_date#44,i_item_desc#45,i_current_price#17,i_wholesale_cost#46,i_brand_id#47,i_brand#48,i_class_id#49,i_class#50,i_category_id#51,i_category#18,i_manufact_id#52,i_manufact#53,i_size#54,i_formulation#55,i_color#56,i_units#57,i_container#58,i_manager_id#59,i_product_name#60] parquet

== Physical Plan ==
TakeOrderedAndProject(limit=100, orderBy=[cnt#5 ASC NULLS FIRST], output=[state#1,cnt#5])
+- *(8) Project [state#1, cnt#5]
   +- *(8) Filter (count(1)#6 >= 10)
      +- *(8) HashAggregate(keys=[ca_state#7], functions=[count(1)], output=[state#1, cnt#5, count(1)#6])
         +- Exchange hashpartitioning(ca_state#7, 5), true, [id=#109]
            +- *(7) HashAggregate(keys=[ca_state#7], functions=[partial_count(1)], output=[ca_state#7, count#110])
               +- *(7) Project [ca_state#7]
                  +- *(7) BroadcastHashJoin [ss_item_sk#14], [i_item_sk#15], Inner, BuildRight, false
                     :- *(7) Project [ca_state#7, ss_item_sk#14]
                     :  +- *(7) BroadcastHashJoin [ss_sold_date_sk#12], [d_date_sk#13], Inner, BuildRight, false
                     :     :- *(7) Project [ca_state#7, ss_sold_date_sk#12, ss_item_sk#14]
                     :     :  +- *(7) BroadcastHashJoin [c_customer_sk#10], [ss_customer_sk#11], Inner, BuildRight, false
                     :     :     :- *(7) Project [ca_state#7, c_customer_sk#10]
                     :     :     :  +- *(7) BroadcastHashJoin [ca_address_sk#8], [c_current_addr_sk#9], Inner, BuildRight, false
                     :     :     :     :- *(7) Project [ca_address_sk#8, ca_state#7]
                     :     :     :     :  +- *(7) Filter isnotnull(ca_address_sk#8)
                     :     :     :     :     +- *(7) ColumnarToRow
                     :     :     :     :        +- FileScan parquet default.customer_address[ca_address_sk#8,ca_state#7] Batched: true, DataFilters: [isnotnull(ca_address_sk#8)], Format: Parquet, Location: InMemoryFileIndex[file:/Users/yi.wu/IdeaProjects/spark/sql/core/spark-warehouse/org.apache.spark...., PartitionFilters: [], PushedFilters: [IsNotNull(ca_address_sk)], ReadSchema: struct<ca_address_sk:int,ca_state:string>
                     :     :     :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[1, int, true] as bigint)),false), [id=#111]
                     :     :     :        +- *(1) Project [c_customer_sk#10, c_current_addr_sk#9]
                     :     :     :           +- *(1) Filter (isnotnull(c_current_addr_sk#9) AND isnotnull(c_customer_sk#10))
                     :     :     :              +- *(1) ColumnarToRow
                     :     :     :                 +- FileScan parquet default.customer[c_customer_sk#10,c_current_addr_sk#9] Batched: true, DataFilters: [isnotnull(c_current_addr_sk#9), isnotnull(c_customer_sk#10)], Format: Parquet, Location: InMemoryFileIndex[file:/Users/yi.wu/IdeaProjects/spark/sql/core/spark-warehouse/org.apache.spark...., PartitionFilters: [], PushedFilters: [IsNotNull(c_current_addr_sk), IsNotNull(c_customer_sk)], ReadSchema: struct<c_customer_sk:int,c_current_addr_sk:int>
                     :     :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[2, int, true] as bigint)),false), [id=#112]
                     :     :        +- *(2) Project [ss_sold_date_sk#12, ss_item_sk#14, ss_customer_sk#11]
                     :     :           +- *(2) Filter ((isnotnull(ss_customer_sk#11) AND isnotnull(ss_sold_date_sk#12)) AND isnotnull(ss_item_sk#14))
                     :     :              +- *(2) ColumnarToRow
                     :     :                 +- FileScan parquet default.store_sales[ss_sold_date_sk#12,ss_item_sk#14,ss_customer_sk#11] Batched: true, DataFilters: [isnotnull(ss_customer_sk#11), isnotnull(ss_sold_date_sk#12), isnotnull(ss_item_sk#14)], Format: Parquet, Location: InMemoryFileIndex[file:/Users/yi.wu/IdeaProjects/spark/sql/core/spark-warehouse/org.apache.spark...., PartitionFilters: [], PushedFilters: [IsNotNull(ss_customer_sk), IsNotNull(ss_sold_date_sk), IsNotNull(ss_item_sk)], ReadSchema: struct<ss_sold_date_sk:int,ss_item_sk:int,ss_customer_sk:int>
                     :     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [id=#113]
                     :        +- *(3) Project [d_date_sk#13]
                     :           +- *(3) Filter ((isnotnull(d_month_seq#16) AND (d_month_seq#16 = Subquery scalar-subquery#3, [id=#114])) AND isnotnull(d_date_sk#13))
                     :              :  +- Subquery scalar-subquery#3, [id=#114]
                     :              :     +- *(2) HashAggregate(keys=[d_month_seq#16], functions=[], output=[d_month_seq#16])
                     :              :        +- Exchange hashpartitioning(d_month_seq#16, 5), true, [id=#115]
                     :              :           +- *(1) HashAggregate(keys=[d_month_seq#16], functions=[], output=[d_month_seq#16])
                     :              :              +- *(1) Project [d_month_seq#16]
                     :              :                 +- *(1) Filter (((isnotnull(d_year#19) AND isnotnull(d_moy#20)) AND (d_year#19 = 2000)) AND (d_moy#20 = 1))
                     :              :                    +- *(1) ColumnarToRow
                     :              :                       +- FileScan parquet default.date_dim[d_month_seq#16,d_year#19,d_moy#20] Batched: true, DataFilters: [isnotnull(d_year#19), isnotnull(d_moy#20), (d_year#19 = 2000), (d_moy#20 = 1)], Format: Parquet, Location: InMemoryFileIndex[file:/Users/yi.wu/IdeaProjects/spark/sql/core/spark-warehouse/org.apache.spark...., PartitionFilters: [], PushedFilters: [IsNotNull(d_year), IsNotNull(d_moy), EqualTo(d_year,2000), EqualTo(d_moy,1)], ReadSchema: struct<d_month_seq:int,d_year:int,d_moy:int>
                     :              +- *(3) ColumnarToRow
                     :                 +- FileScan parquet default.date_dim[d_date_sk#13,d_month_seq#16] Batched: true, DataFilters: [isnotnull(d_month_seq#16), isnotnull(d_date_sk#13)], Format: Parquet, Location: InMemoryFileIndex[file:/Users/yi.wu/IdeaProjects/spark/sql/core/spark-warehouse/org.apache.spark...., PartitionFilters: [], PushedFilters: [IsNotNull(d_month_seq), IsNotNull(d_date_sk)], ReadSchema: struct<d_date_sk:int,d_month_seq:int>
                     +- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, true] as bigint)),false), [id=#116]
                        +- *(6) Project [i_item_sk#15]
                           +- *(6) Filter (cast(i_current_price#17 as decimal(14,7)) > CheckOverflow((1.200000 * promote_precision(avg(i_current_price)#41)), DecimalType(14,7), true))
                              +- *(6) BroadcastHashJoin [i_category#18], [i_category#18#108], LeftOuter, BuildRight, false
                                 :- *(6) Project [i_item_sk#15, i_current_price#17, i_category#18]
                                 :  +- *(6) Filter (isnotnull(i_current_price#17) AND isnotnull(i_item_sk#15))
                                 :     +- *(6) ColumnarToRow
                                 :        +- FileScan parquet default.item[i_item_sk#15,i_current_price#17,i_category#18] Batched: true, DataFilters: [isnotnull(i_current_price#17), isnotnull(i_item_sk#15)], Format: Parquet, Location: InMemoryFileIndex[file:/Users/yi.wu/IdeaProjects/spark/sql/core/spark-warehouse/org.apache.spark...., PartitionFilters: [], PushedFilters: [IsNotNull(i_current_price), IsNotNull(i_item_sk)], ReadSchema: struct<i_item_sk:int,i_current_price:decimal(7,2),i_category:string>
                                 +- BroadcastExchange HashedRelationBroadcastMode(List(input[1, string, true]),false), [id=#117]
                                    +- *(5) HashAggregate(keys=[i_category#18], functions=[avg(UnscaledValue(i_current_price#17))], output=[avg(i_current_price)#41, i_category#18#108])
                                       +- Exchange hashpartitioning(i_category#18, 5), true, [id=#118]
                                          +- *(4) HashAggregate(keys=[i_category#18], functions=[partial_avg(UnscaledValue(i_current_price#17))], output=[i_category#18, sum#119, count#120])
                                             +- *(4) Project [i_current_price#17, i_category#18]
                                                +- *(4) Filter isnotnull(i_category#18)
                                                   +- *(4) ColumnarToRow
                                                      +- FileScan parquet default.item[i_current_price#17,i_category#18] Batched: true, DataFilters: [isnotnull(i_category#18)], Format: Parquet, Location: InMemoryFileIndex[file:/Users/yi.wu/IdeaProjects/spark/sql/core/spark-warehouse/org.apache.spark...., PartitionFilters: [], PushedFilters: [IsNotNull(i_category)], ReadSchema: struct<i_current_price:decimal(7,2),i_category:string>
